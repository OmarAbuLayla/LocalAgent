{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local Coding Agent (Kaggle Notebook)\n",
        "This notebook adds the agent runtime, tools, FastAPI endpoints, and ngrok wiring.\n",
        "Model loading is assumed to be available (tokenizer + model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies (skip if already installed)\n",
        "!pip -q install -U \\\n",
        "  fastapi uvicorn pyngrok \\\n",
        "  transformers bitsandbytes accelerate sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Imports + config\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "import threading\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "WORKSPACE_DIR = \"/kaggle/working/workspace\"\n",
        "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
        "\n",
        "API_PORT = 8000\n",
        "DEFAULT_CMD_TIMEOUT = 120\n",
        "\n",
        "print(f\"Workspace: {WORKSPACE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Validate model/tokenizer are loaded (skip if already done)\n",
        "try:\n",
        "    tokenizer\n",
        "    model\n",
        "    print(\"Model and tokenizer already loaded.\")\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Load the model and tokenizer before running the agent cells.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Tools (safe file IO + command execution)\n",
        "def _safe_path(path: str) -> str:\n",
        "    if not path:\n",
        "        raise ValueError(\"Path is required.\")\n",
        "    if path.startswith(\"/\"):\n",
        "        raise ValueError(\"Absolute paths are not allowed.\")\n",
        "    normalized = os.path.normpath(os.path.join(WORKSPACE_DIR, path))\n",
        "    if not normalized.startswith(WORKSPACE_DIR):\n",
        "        raise ValueError(\"Path escapes workspace.\")\n",
        "    return normalized\n",
        "\n",
        "def read_file(path: str) -> str:\n",
        "    full_path = _safe_path(path)\n",
        "    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def write_file(path: str, content: str) -> str:\n",
        "    full_path = _safe_path(path)\n",
        "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
        "    with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    return f\"Wrote {len(content)} bytes to {path}\"\n",
        "\n",
        "def list_dir(path: str) -> str:\n",
        "    target = path or \".\"\n",
        "    full_path = _safe_path(target)\n",
        "    return \"\\n\".join(sorted(os.listdir(full_path)))\n",
        "\n",
        "_BLOCKED_PATTERNS = [\n",
        "    \"rm -rf\",\n",
        "    \"sudo\",\n",
        "    \"shutdown\",\n",
        "    \"reboot\",\n",
        "    \"mkfs\",\n",
        "    \"dd\",\n",
        "    \":(){:|:&};:\",\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class ProcInfo:\n",
        "    pid: int\n",
        "    cmd: str\n",
        "    port: Optional[int]\n",
        "    process: subprocess.Popen\n",
        "\n",
        "_processes: Dict[str, ProcInfo] = {}\n",
        "_tunnels: Dict[int, str] = {}\n",
        "\n",
        "def _is_blocked(cmd: str) -> bool:\n",
        "    lowered = cmd.lower()\n",
        "    return any(pat in lowered for pat in _BLOCKED_PATTERNS)\n",
        "\n",
        "def run_cmd(cmd: str, timeout: int = DEFAULT_CMD_TIMEOUT, background: bool = False) -> Dict[str, Any]:\n",
        "    if _is_blocked(cmd):\n",
        "        return {\n",
        "            \"stdout\": \"\",\n",
        "            \"stderr\": \"Blocked potentially destructive command.\",\n",
        "            \"exit_code\": 1,\n",
        "        }\n",
        "    try:\n",
        "        if background:\n",
        "            process = subprocess.Popen(\n",
        "                cmd,\n",
        "                cwd=WORKSPACE_DIR,\n",
        "                shell=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True,\n",
        "            )\n",
        "            proc_id = f\"proc-{process.pid}\"\n",
        "            _processes[proc_id] = ProcInfo(pid=process.pid, cmd=cmd, port=None, process=process)\n",
        "            return {\n",
        "                \"stdout\": \"\",\n",
        "                \"stderr\": \"\",\n",
        "                \"exit_code\": 0,\n",
        "                \"process_id\": proc_id,\n",
        "            }\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            cwd=WORKSPACE_DIR,\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "        return {\n",
        "            \"stdout\": result.stdout,\n",
        "            \"stderr\": result.stderr,\n",
        "            \"exit_code\": result.returncode,\n",
        "        }\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return {\n",
        "            \"stdout\": \"\",\n",
        "            \"stderr\": f\"Command timed out after {timeout}s.\",\n",
        "            \"exit_code\": 124,\n",
        "        }\n",
        "    except Exception as exc:\n",
        "        return {\n",
        "            \"stdout\": \"\",\n",
        "            \"stderr\": str(exc),\n",
        "            \"exit_code\": 1,\n",
        "        }\n",
        "\n",
        "def stop_process(proc_id: str) -> Dict[str, Any]:\n",
        "    info = _processes.get(proc_id)\n",
        "    if not info:\n",
        "        return {\n",
        "            \"stdout\": \"\",\n",
        "            \"stderr\": \"Process not found.\",\n",
        "            \"exit_code\": 1,\n",
        "        }\n",
        "    info.process.terminate()\n",
        "    info.process.wait(timeout=10)\n",
        "    _processes.pop(proc_id, None)\n",
        "    return {\n",
        "        \"stdout\": f\"Stopped {proc_id}\",\n",
        "        \"stderr\": \"\",\n",
        "        \"exit_code\": 0,\n",
        "    }\n",
        "\n",
        "def ensure_ngrok_tunnel(port: int) -> str:\n",
        "    if port in _tunnels:\n",
        "        return _tunnels[port]\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "    _tunnels[port] = public_url\n",
        "    print(f\"ngrok tunnel for {port}: {public_url}\")\n",
        "    return public_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Agent prompt + tool parsing\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are DevAgent, a local coding agent optimized for frontend development (React, Next.js, Vite, TS, CSS).\n",
        "You must emit tool calls using EXACTLY this format:\n",
        "\n",
        "TOOL: <tool_name>\n",
        "ARGS:\n",
        "<arguments as plain text>\n",
        "\n",
        "Tool args can be either:\n",
        "- A plain text blob (e.g., run_cmd command)\n",
        "- OR key=value lines, e.g.:\n",
        "  path=src/App.tsx\n",
        "  content=...file contents...\n",
        "\n",
        "Available tools: read_file, write_file, list_dir, run_cmd.\n",
        "Rules:\n",
        "- Only use paths inside /kaggle/working/workspace.\n",
        "- Avoid destructive commands (rm -rf, sudo, shutdown, reboot, mkfs, dd, fork bombs).\n",
        "- Use tools for file or shell interactions.\n",
        "\"\"\".strip()\n",
        "\n",
        "conversation_history: List[Dict[str, str]] = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
        "]\n",
        "\n",
        "def _parse_key_values(text: str) -> Dict[str, str]:\n",
        "    result = {}\n",
        "    for line in text.splitlines():\n",
        "        if \"=\" in line:\n",
        "            key, value = line.split(\"=\", 1)\n",
        "            result[key.strip()] = value\n",
        "    return result\n",
        "\n",
        "def _extract_tool_calls(text: str) -> List[Dict[str, str]]:\n",
        "    pattern = re.compile(r\"TOOL:\\s*(\\w+)\\s*\\nARGS:\\s*([\\s\\S]*?)(?=\\nTOOL:|$)\")\n",
        "    calls = []\n",
        "    for match in pattern.finditer(text):\n",
        "        calls.append({\n",
        "            \"tool\": match.group(1).strip(),\n",
        "            \"args\": match.group(2).strip(),\n",
        "        })\n",
        "    return calls\n",
        "\n",
        "def _format_tool_result(tool_name: str, result: Any) -> str:\n",
        "    payload = result if isinstance(result, str) else json.dumps(result, indent=2)\n",
        "    return f\"TOOL_RESULT: {tool_name}\\n{payload}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Agent loop\n",
        "def _generate_model_reply(messages: List[Dict[str, str]]) -> str:\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.2,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    generated = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
        "\n",
        "def _call_tool(tool_name: str, args: str) -> Tuple[Any, str]:\n",
        "    parsed = _parse_key_values(args)\n",
        "    if tool_name == \"read_file\":\n",
        "        path = parsed.get(\"path\", args)\n",
        "        return read_file(path), \"\"\n",
        "    if tool_name == \"write_file\":\n",
        "        if parsed:\n",
        "            path = parsed.get(\"path\", \"\")\n",
        "            content = parsed.get(\"content\", \"\")\n",
        "        else:\n",
        "            path, content = args.split(\"\\n\", 1)\n",
        "        return write_file(path.strip(), content), \"\"\n",
        "    if tool_name == \"list_dir\":\n",
        "        path = parsed.get(\"path\", args)\n",
        "        return list_dir(path.strip()), \"\"\n",
        "    if tool_name == \"run_cmd\":\n",
        "        cmd = parsed.get(\"cmd\", args)\n",
        "        timeout = int(parsed.get(\"timeout\", DEFAULT_CMD_TIMEOUT))\n",
        "        background = parsed.get(\"background\", \"false\").lower() == \"true\"\n",
        "        return run_cmd(cmd, timeout=timeout, background=background), \"\"\n",
        "    return {\"error\": f\"Unknown tool: {tool_name}\"}, \"\"\n",
        "\n",
        "def run_agent(user_message: str, max_loops: int = 6) -> Dict[str, str]:\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
        "    logs: List[str] = []\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        assistant_reply = _generate_model_reply(conversation_history)\n",
        "        tool_calls = _extract_tool_calls(assistant_reply)\n",
        "        if not tool_calls:\n",
        "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "            return {\"reply\": assistant_reply, \"logs\": \"\\n\\n\".join(logs)}\n",
        "\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "        for call in tool_calls:\n",
        "            result, _ = _call_tool(call[\"tool\"], call[\"args\"])\n",
        "            log_entry = _format_tool_result(call[\"tool\"], result)\n",
        "            logs.append(log_entry)\n",
        "            conversation_history.append({\"role\": \"user\", \"content\": log_entry})\n",
        "\n",
        "    return {\n",
        "        \"reply\": \"Agent stopped after reaching max tool loops.\",\n",
        "        \"logs\": \"\\n\\n\".join(logs),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: FastAPI endpoints\n",
        "app = FastAPI()\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "\n",
        "class FileRequest(BaseModel):\n",
        "    path: str\n",
        "    content: str\n",
        "\n",
        "class CmdRequest(BaseModel):\n",
        "    cmd: str\n",
        "    timeout: Optional[int] = DEFAULT_CMD_TIMEOUT\n",
        "    background: Optional[bool] = False\n",
        "\n",
        "class ServerRequest(BaseModel):\n",
        "    cmd: str\n",
        "    port: int\n",
        "\n",
        "class StopRequest(BaseModel):\n",
        "    process_id: str\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "@app.post(\"/reset\")\n",
        "def reset():\n",
        "    conversation_history.clear()\n",
        "    conversation_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
        "    return {\"status\": \"reset\"}\n",
        "\n",
        "@app.get(\"/files\")\n",
        "def files(path: str = \".\"):\n",
        "    return {\"entries\": list_dir(path)}\n",
        "\n",
        "@app.get(\"/file\")\n",
        "def get_file(path: str):\n",
        "    return {\"content\": read_file(path)}\n",
        "\n",
        "@app.post(\"/file\")\n",
        "def put_file(req: FileRequest):\n",
        "    return {\"result\": write_file(req.path, req.content)}\n",
        "\n",
        "@app.post(\"/cmd\")\n",
        "def cmd(req: CmdRequest):\n",
        "    return run_cmd(req.cmd, timeout=req.timeout or DEFAULT_CMD_TIMEOUT, background=bool(req.background))\n",
        "\n",
        "@app.post(\"/run_server\")\n",
        "def run_server(req: ServerRequest):\n",
        "    result = run_cmd(req.cmd, timeout=DEFAULT_CMD_TIMEOUT, background=True)\n",
        "    process_id = result.get(\"process_id\")\n",
        "    if not process_id:\n",
        "        return result\n",
        "    proc_info = _processes.get(process_id)\n",
        "    if proc_info:\n",
        "        proc_info.port = req.port\n",
        "    app_url = ensure_ngrok_tunnel(req.port)\n",
        "    return {\n",
        "        **result,\n",
        "        \"app_url\": app_url,\n",
        "    }\n",
        "\n",
        "@app.post(\"/stop_server\")\n",
        "def stop_server(req: StopRequest):\n",
        "    return stop_process(req.process_id)\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat(req: ChatRequest):\n",
        "    return run_agent(req.message)\n",
        "\n",
        "def _run_api():\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=API_PORT, log_level=\"info\")\n",
        "\n",
        "api_thread = threading.Thread(target=_run_api, daemon=True)\n",
        "api_thread.start()\n",
        "\n",
        "print(f\"FastAPI running on port {API_PORT}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: ngrok tunnel for the API\n",
        "ngrok_auth_token = os.getenv(\"NGROK_AUTHTOKEN\")\n",
        "if ngrok_auth_token:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "api_public_url = ensure_ngrok_tunnel(API_PORT)\n",
        "print(f\"Agent API URL: {api_public_url}\")\n",
        "print(\"Use POST /chat on this URL.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
